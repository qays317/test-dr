# ğŸš€ AWS Multi-Region WordPress Disaster Recovery Architecture


[![Deploy Multi-Region Infrastructure](https://github.com/QaysAlnajjad/aws-multi-region-wordpress-dr/actions/workflows/deploy.yml/badge.svg)](https://github.com/QaysAlnajjad/aws-multi-region-wordpress-dr/actions/workflows/deploy.yml)

[![Destroy Multi-Region Infrastructure](https://github.com/QaysAlnajjad/aws-multi-region-wordpress-dr/actions/workflows/destroy.yml/badge.svg)](https://github.com/QaysAlnajjad/aws-multi-region-wordpress-dr/actions/workflows/destroy.yml)


**Production-Grade â€¢ Highly Available â€¢ Fault-Tolerant â€¢ Terraform & AWS**

This repository delivers a **real-world enterprise disaster recovery design** for running WordPress across **two AWS regions** using a fully automated, highly available, self-healing architecture.

All infrastructure is 100% managed using **Terraform**, following AWS **Well-Architected best practices**.

---
# ğŸ“˜ Table of Contents

- [Architecture Overview](#architecture-overview)
- [Key Features](#key-features)
- [Design Principles](#design-principles)
- [Technology Stack](#technology-stack)
- [Infrastructure Components](#infrastructure-components)
- [Failover Strategy](#failover-strategy)
- [Terraform Structure](#terraform-structure)
- [Reviewer Setup (How to Deploy This Project in Your AWS Account)](#reviewer-setup-how-to-deploy-this-project-in-your-aws-account-)
- [DR Failover Guide](#dr-failover-guide)
- [CloudWatch Monitoring and Alarms](#cloudwatch-monitoring-and-alarms)
- [Security Best Practices Used](#security-best-practices-used)
- [Cost Optimization](#cost-optimization)
- [Known Limitations and Trade-offs](#known-limitations-and-trade-offs)
- [License](#license)

---

# ğŸ— **Architecture Overview**

This project deploys a multi-region, production-grade WordPress platform using:

* **Primary Region (Active):** `us-east-1`
* **DR Region (Warm Standby):** `ca-central-1`
* **Global routing:** One CloudFront distribution + Route 53
* **Containers:** ECS Fargate (Primary active, DR scaled to 0)
* **Database:** RDS MySQL with cross-region read-replica
* **Media Storage:** S3 with cross-region replication
* **Origin Failover:** CloudFront automatically fails over to DR ALB & DR S3
--- 

## ğŸ— Multi-Region Architecture (ASCII Diagram)

```text
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚   Route 53   â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚      CloudFront (Global)      â”‚
                            â”‚  Origin Groups (Auto Failover)â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚           â”‚
                                      App         Media
                                 (Dynamic)      (Uploads)
                                        â”‚           â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   ALB Primary (us-east-1)â”‚ â”‚  S3 Primary (us-east-1)   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚                               â”‚
                        ECS Tasks (Active)               Media Writes
                    Scale: 2 Tasks (Example)            Replication to DR
                             â”‚                               â”‚
                             â”‚                               â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   ALB DR (ca-central-1)  â”‚ â”‚   S3 DR (ca-central-1)    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                        ECS Tasks (Standby)
                      Scale: 0 Tasks (Normal)

           Database Layer
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   RDS Primary (Writer)    â”‚
           â”‚        us-east-1          â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                 Replication (Async)
                        â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  RDS Read Replica (DR)    â”‚
           â”‚      ca-central-1         â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          
```

---

# ğŸ¥ Demo Videos

The following short demos walk through the full lifecycle of the project â€” from bootstrap and deployment to validation and disaster recovery simulation.

### â–¶ï¸ **Demo 1 (8 min)**  
**AWS Multi-Region WordPress DR â€“ Bootstrap &  CI/CD Deployment (Terraform + OIDC)**  
https://youtu.be/-jRgfdVbeQc

Covers:
- Bootstrap IAM stack (OIDC, no static credentials)
- GitHub Actions â†’ AWS authentication
- Automated multi-region Terraform deployment

---

### â–¶ï¸ **Demo 2 (4 min)**  
**AWS Multi-Region WordPress DR â€“ Primary Region Validation**  
https://youtu.be/DITJBaNBQeQ

Covers:
- WordPress frontend & admin access
- ALB + CloudFront behavior
- S3 media uploads
- RDS primary connectivity

---

### â–¶ï¸ **Demo 3 (10 min)**  
**AWS Multi-Region WordPress DR â€“ Failover Simulation**  
https://youtu.be/7cX2Dw5RYVM

Covers:
- Primary region failure simulation
- CloudFront origin failover
- DR ALB traffic serving
- RDS replica promotion
- ECS scale-up in DR region
- S3 media uploads (in DR region)

---

> â„¹ï¸ **Post-Demo Improvement Note**
>
> The demo videos were recorded before a final security enhancement was applied.
>
> At the time of recording, the WordPress admin interface (`admin.example.com/wp-admin`)
> triggered a browser SSL warning because the ACM certificate covered only the root domain.
>
> **After recording the demos**, the certificate configuration was improved to cover:
>
> ```
> *.example.com
> ```
>
> This eliminates all SSL warnings and ensures secure HTTPS access for both:
> - Frontend traffic
> - WordPress admin access
>
> The current code in this repository reflects the **improved and correct behavior**.

---

> ğŸ” **Security Note**  
> All demos were recorded using a sandbox AWS account with GitHub Actions OIDC authentication.  
> No static AWS credentials, secrets, or IAM users were used or exposed.  
> All IAM roles, infrastructure, and credentials used during recording were destroyed immediately after the demos.

---

# â­ **Key Features**

### ğŸŸ¢ High Availability & Automated Failover

* Multi-region ECS + ALB
* Cross-region database replication
* CloudFront origin failover with zero DNS changes or propagation delay

### ğŸŒ Global Content Delivery

* S3 + CloudFront for media
* Uploads served from nearest edge location

### ğŸ”’ Hardened Security

* TLS everywhere
* Secrets in AWS Secrets Manager
* IAM-role access for WordPress S3 integration
* Private subnets, VPC endpoints, strict SGs

### âš™ï¸ Fully Automated with Terraform

* Modular structure
* Remote state per environment
* Zero manual configuration

---

# ğŸ“ **Design Principles**

| AWS Well-Architected Pillar | Implementation                                         |
| --------------------------- | ------------------------------------------------------ |
| **Reliability**             | Multi-region, auto failover, RDS replica               |
| **Security**                | HTTPS, IAM roles, secrets manager, least-privilege SGs |
| **Performance**             | CloudFront CDN, S3 media, Fargate                      |
| **Cost-Optimization**       | Warm standby DR, endpoints to reduce NAT traffic       |
| **Operational Excellence**  | Full IaC, zero manual provisioning                     |

---

# ğŸ”§ **Technology Stack**

### **AWS Services**

* ECS Fargate
* RDS MySQL (Multi-Region)
* S3 (Primary + DR)
* CloudFront CDN
* ALB
* Route 53
* Secrets Manager
* VPC + Endpoints
* CloudWatch + Logs
* ACM (provided or auto-generated)

### **Application Stack**

* WordPress
* WP-CLI
* Amazon S3 / CloudFront plugin
* Hardened `wp-config.php`
* Custom Docker image

---

# ğŸ§± **Infrastructure Components**

### ğŸŸ¦ **1. ECS Fargate WordPress**

* Stateless containers
* Auto-healing
* No EC2 management
* Custom Dockerfile:

  * WP installed via WP-CLI
  * S3 plugin auto-configured
  * Admin URL rewriting
  * HTTPS detection (for CloudFront/ALB)

---

### ğŸŸ© **2. Application Load Balancer**

* HTTPS termination
* Health checks used by CloudFront failover
* Admin subdomain bypasses CloudFront and routes directly to the ALB

---

### ğŸŸ¥ **3. CloudFront Distribution**

* Two origin groups:

  1. **ALB Primary â†’ ALB DR**
  2. **S3 Primary â†’ S3 DR**
* Default: application traffic
* Ordered: WordPress media uploads
* Full automatic failover
* TLS enabled using ACM

---

### ğŸŸ¨ **4. RDS MySQL**

* Primary RDS
* DR region read-replica
* Manual promotion during primary region failure

---

### ğŸŸ« **5. S3 Media Storage**

* Two buckets: Primary + DR
* WordPress uploads to primary bucket
* Cross-Region Replication automatically sends objects to DR bucket
* CloudFront serves media via S3 origin group failover
* ECS uses IAM role + VPC endpoint for secure access

Media failover is 100% automatic - no operator intervention needed.

---

### ğŸŸª **6. VPC + Networking**

* Private ECS subnets
* Public ALB subnets
* NAT Gateway minimized
* VPC Endpoints:

  * S3
  * ECR
  * Logs
  * Secrets Manager
  * CloudWatch

* Each region has its own isolated VPC to ensure true regional independence.

---

# ğŸŒ **Failover Strategy**

## **1. Application Failover (Fully Automatic)**

CloudFront Origin Group:

```
Primary ALB â†’ DR ALB
```

Triggers failover on:

* error 5xx
* Timeout
* ALB unreachable
* Security group or NACL issues

**Users experience zero downtime**.

---

## **2. Media Failover**

CloudFront S3 Origin Group:

```
Primary S3 â†’ DR S3
```

Read failover is automatic.
Write failover is controlled at ECS task-level.

---

## **3. Database Failover (RDS â†’ DR Region)**

### Default (manual):

* Amazon RDS MySQL (Primary Region)
* Cross-Region Read Replica (DR Region)
* AWS Secrets Manager per region (Primary secret, DR secret)
* ECS Tasks in each region automatically read the correct secret

---

## **4. ECS Failover**

### **Primary Region**
- Runs full production ECS service (ex: 2 tasks)
- Serves all user traffic under normal conditions

### **DR Region (Warm Standby)**
- ECS service is fully deployed but scaled down to 0 tasks.
- This keeps costs minimal while ensuring the infrastructure is ready.

### **Failover Process**
When the primary region becomes unavailable:

1. **CloudFront automatically fails over** to the DR ALB.
2. The DR ECS service is **manually scaled** (or via automation) from 0 to 2 tasks.
3. DR tasks start, register with the DR target group, and immediately begin serving traffic.

This architecture follows AWS Warm Standby DR pattern â€” a cost-efficient model where the secondary region remains ready but scaled down until failover.

---

# ğŸ“ **Terraform Structure**

```bash
aws-disaster-recovery/
â”‚
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ bootstrap/
â”‚   â”œâ”€â”€ global/
â”‚   â”‚   â”œâ”€â”€ iam/
â”‚   â”‚   â”œâ”€â”€ oac/  
â”‚   â”‚   â”œâ”€â”€ cdn_dns/
â”‚   â”œâ”€â”€ primary/
â”‚   â”‚   â”œâ”€â”€ network_rds/
â”‚   â”‚   â”œâ”€â”€ s3/
â”‚   â”‚   â”œâ”€â”€ alb/
â”‚   â”‚   â”œâ”€â”€ ecs/     
â”‚   â””â”€â”€ dr/
â”‚       â”œâ”€â”€ network/
â”‚       â”œâ”€â”€ read_replica_rds/
â”‚       â”œâ”€â”€ s3/
â”‚       â”œâ”€â”€ alb/
â”‚       â””â”€â”€ ecs/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ acm/
â”‚   â”œâ”€â”€ alb/
â”‚   â”œâ”€â”€ cdn_dns/
â”‚   â”œâ”€â”€ ecs/
â”‚   â”œâ”€â”€ iam/
â”‚   â”œâ”€â”€ rds/
â”‚   â”œâ”€â”€ s3/
â”‚   â”œâ”€â”€ sg/
â”‚   â””â”€â”€ vpc
â””â”€â”€ scripts/
    â””â”€â”€ deployment-automation-scripts/
    â”‚   â”œâ”€â”€ config.sh
    â”‚   â”œâ”€â”€ deploy.sh
    â”‚   â”œâ”€â”€ destroy.sh
    â”‚   â”œâ”€â”€ pull-docker-hub-to-ecr.sh
    â”‚   â””â”€â”€ stacks_config.sh
    â””â”€â”€ runtime/
        â”œâ”€â”€ primary-ecr-image-uri
        â””â”€â”€ dr-ecr-image-uri   
```
This structure prevents dependency cycles and allows independent region deployments.

---

## Cross-Stack Dependency Map

This project uses multiple Terraform stacks.  
A detailed diagram of how stack outputs flow between stacks is available here:

ğŸ‘‰ [Cross-Stack Variable Flow](docs/cross-stack-flow.md)

--- 

# ğŸ“˜ **Reviewer Setup (How to Deploy This Project in Your AWS Account)** 

This section explains exactly how to deploy and test the full multi-region WordPress DR architecture in your own AWS account, with no AWS access keys and no manual Terraform commands (after bootstrap).

The setup is intentionally simple and follows AWS + GitHub industry CI/CD patterns.
## âœ… 1. Requirements

You need:

âœ” AWS account
with permissions to create IAM, VPC, ECS, RDS, S3, CloudFront, ALB, Route53.

âœ” A Route53 hosted zone
for your domain (example: yourdomain.com).

âœ” (Optional) ACM certificates
If you donâ€™t provide them, the infrastructure will create them automatically.

## ğŸš€ 2. Clone the Project

No fork needed:
```bash
git clone https://github.com/QaysAlnajjad/aws-multi-region-wordpress-dr.git
cd aws-multi-region-wordpress-dr
```

## ğŸŸ¦ 3. Deploy the Bootstrap Stack (ONE TIME ONLY)

This step enables GitHub Actions OIDC â†’ AWS IAM, allowing GitHub to deploy in your AWS account without any access keys.

âœ” What bootstrap creates:

|              Resource                      |                            Purpose                                   |
| ------------------------------------------ | -------------------------------------------------------------------- |
| AWS IAM OpenID Connect Provider (GitHub)   | Allow GitHub Actions to authenticate to AWS                          |
| GitHub Actions IAM role                    | This is assumed by the deploy/destroy workflows                      |
| Trust policy restricted to the repository  | security-hardening: only our repository can use this role            |
| AdministratorAccess policy                 | Full deploy/destroy capabilities (reviewer may restrict this later)  |

âš ï¸ Important â€” Update Repository Name Before Deploying Bootstrap

The IAM trust policy created in this step is restricted to a single GitHub repository.
If you cloned this project into your own GitHub account, you MUST update the repository name in this file:
```bash
environments/bootstrap/main.tf
```
find:
```bash
"token.actions.githubusercontent.com:sub" = "repo:qays317/review-test:*"
```
â¡ Replace with your repository identifier:
```bash
repo:<username>/<repository-name>
```
If you donâ€™t update this, GitHub Actions will fail with:

Not authorized to assume role

Step 3.1 â€” Authenticate to AWS locally

Either:
```bash
aws configure
```
or
```bash
export AWS_ACCESS_KEY_ID=xxxx
export AWS_SECRET_ACCESS_KEY=xxxx
export AWS_DEFAULT_REGION=us-east-1
```
Step 3.2 â€” Deploy bootstrap
```bash
terraform -chdir=environments/bootstrap init
terraform -chdir=environments/bootstrap apply
```
You will receive an output:
```bash
github_actions_role_arn = arn:aws:iam::<ACCOUNT-ID>:role/github-actions-terraform-role
```

## ğŸŸ© 4. Add the Role ARN to GitHub Actions Workflows

You do NOT need GitHub secrets.

Just open:
```bash
.github/workflows/deploy.yml  
.github/workflows/destroy.yml
```
Find:
```bash
role-to-assume: arn:aws:iam::<ACCOUNT-ID>:role/github-actions-terraform-role
```
Replace <ACCOUNT-ID> with your AWS account ID.

âœ” This is all GitHub needs.
âœ” No secrets.
âœ” No PAT.
âœ” No long-lived keys.
âœ” Secure and industry-standard.

## ğŸŸ§ 5. Configure Deployment Parameters

Open:
```bash
scripts/deployment-automation-scripts/config.sh
```
Edit values to match your AWS environment:
| Variable                        | Purpose                                                                               |
|---------------------------------|---------------------------------------------------------------------------------------|
| PRIMARY_REGION                  | AWS region for the primary deployment (e.g., us-east-1)                               | 
| DR_REGION                       | AWS region for the DR deployment (e.g., ca-central-1)                                 |
| TF_STATE_BUCKET_NAME            | S3 bucket used for ALL Terraform remote state                                         |
| TF_STATE_BUCKET_REGION          | Region of the Terraform state bucket                                                  | 
| PRIMARY_DOMAIN                  | Root domain (e.g., example.com)                                                       |
| HOSTED_ZONE_ID                  | Route53 hosted zone ID for the domain                                                 |
| PRIMARY_MEDIA_S3_BUCKET         | Name of primary S3 bucket for media                                                   |
| DR_MEDIA_S3_BUCKET              | Name of DR S3 media bucket                                                            |
| PRIMARY_ALB_SSL_CERTIFICATE_ARN | ARN of primary ALB ACM certificate (empty = auto-create with ACM module)              |
| DR_ALB_SSL_CERTIFICATE_ARN      | ARN of DR ALB ACM certificate (empty = auto-create with ACM module)                   |
| CLOUDFRONT_SSL_CERTIFICATE_ARN  | ACM certificate ARN in us-east-1 for CloudFront (empty = auto-create with ACM module) |
| CERTIFICATE_SANs                | Alternative subjects names for SSL certificate                                        |

If you leave any certificate ARN empty, Terraform automatically creates certificates for you.

## ğŸš€ 6. Deploy the Multi-Region Infrastructure

From GitHub â†’ Actions:

âœ” Go to:

Deploy AWS Disaster Recovery â†’ Run workflow

GitHub will automatically:

âœ“ Assume the IAM role you created

âœ“ Load config.sh

âœ“ Mirror the Docker image â†’ ECR (Primary + DR)

âœ“ Deploy the Primary region

âœ“ Deploy the DR region

âœ“ Deploy CloudFront + Route53 global stack

âœ“ Output WordPress endpoints

Total time ~ 50 minutes


### âœ“ ğŸ³ Docker Image Mirroring (Helper Script)

During deployment, the main script:

```bash
scripts/deployment-automation-scripts/deploy.sh
```
internally calls the helper script:
```bash
scripts/deployment-automation-scripts/pull-docker-hub-to-ecr.sh <aws-region> <environment>
```
This helper script is fully automated and:

1. Pulls the WordPress image from Docker Hub defined in config.sh (DOCKERHUB_IMAGE)
2. Ensures the ECR repository exists (ECR_REPO_NAME)
3. Tags and pushes the image to:
```bash
<ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com/ecs-wordpress-app:<TAG>
```
4. Writes the final ECR image URI to:
```bash
scripts/runtime/primary-ecr-image-uri
scripts/runtime/dr-ecr-image-uri
```
The ECS task definitions in both regions then read the correct image URI from these runtime files, so the reviewer does not need to manage image tags manually.

### ğŸ” WordPress Admin Credentials

After the infrastructure deployment completes and WordPress is fully initialized, you can log in to the WordPress admin dashboard using the following 

**default credentials**:

```text
Username: admin
Password: Admin123!
```
**Login URL**:

```text
https://<your-domain>/wp-admin
```

---

## ğŸ’£ 8. Destroy the Infrastructure

From GitHub â†’ Actions:

Destroy AWS Disaster Recovery â†’ Run workflow

This destroys resources in the correct dependency order:

* ECS
* ALBs
* RDS
* VPC
* CloudFront + Route53
* Cleanup ECR pushed images
* Remove runtime state

This ensures a clean teardown with no orphaned resources.

---

# ğŸ†˜ **DR Failover Guide**

### Automatic:

âœ” CloudFront routes traffic to DR ALB
âœ” S3 read failover
âœ” WordPress stays online

### Manual:

1. Promote DR RDS replica
2. Scale ECS tasks in DR region
3. Update S3 write origin (only if primary S3 is down)
4. Post-incident: re-establish replication. After the primary region is restored, the old primary RDS instance must be replaced and a new cross-region read replica must be created to re-establish multi-region replication.

---

# ğŸ“Š **CloudWatch Monitoring and Alarms**

This project implements centralized observability using Amazon CloudWatch.
Both the application (ECS) layer and the automation layer (Lambda) are instrumented with log groups and health-monitoring alarms.

âœ” ECS Logging

Each ECS task writes logs to a dedicated CloudWatch Log Group:
* Log group name pattern:
"/ecs/<task-family-name>"
* Logs retained for 7 days
* Automatically created for each ECS task definition via Terraform

This provides container-level logs for debugging application issues, deployment behavior, or failover events.

âœ” ECS Health Alarm (via ALB Target Group)

To ensure service health and detect issues early, a CloudWatch alarm is configured for the Application Load Balancer (ALB) Target Group:
* Alarm name: wordpress-health-alarm
* Metric: HealthyHostCount (AWS/ApplicationELB)
* Trigger:
  * Alarm fires when healthy container count drops below 2
  * Metric evaluated every 60 seconds
  * treat_missing_data = "breaching" ensures that missing ALB metrics during failure also trigger the alarm
* Dimensions:
  * TargetGroup ARN suffix
  * LoadBalancer ARN suffix
* Purpose:
 * Detects failing ECS tasks or unhealthy containers behind the ALB, enabling early detection of service degradation

âœ” Lambda Execution Logging

The database-initialization Lambda function (wordpress-db-setup) includes full log coverage:
* Log group:
  * "/aws/lambda/wordpress-db-setup"
* Log retention: 7 days
* All Lambda execution output (including errors, DB setup output, Secrets Manager interactions) is logged
* Logs assist in debugging database bootstrap, credential provisioning, and RDS post-creation automation

The database-initialization Lambda function is triggered automatically during the initial deployment process to perform one-time WordPress database bootstrap operations (schema creation and credential initialization).

This automation is intentionally designed to run only during initial provisioning. Future schema changes or migrations are expected to be handled via application-level processes or CI/CD pipelines.

---

# ğŸ” **Security Best Practices Used**

* TLS 1.2+ enforced
* HTTPS for admin + frontend
* Private database
* Security Groups use least privilege
* Secrets stored in Secrets Manager
* IAM roles used instead of access keys
* S3 buckets private (CloudFront handles access)
* Apache SSL disabled inside container (ALB handles TLS)

---

# ğŸ’° **Cost Optimization**

This architecture follows a Warm Standby DR model to significantly reduce multi-region cloud expenses.

Primary Region (Active) â€” Estimated Monthly Cost
| Component  | Service                              | Approx Monthly Cost       |
| ---------- |--------------------------------------|---------------------------|
| Compute    | ECS Fargate tasks (1â€“2 tasks)        | $40â€“$80                   |
| Database   | RDS MySQL (db.t3.medium)             | $120â€“$150                 |  
| Storage    | S3 buckets + backups                 | $5â€“$15                    |
| Networking | ALB + VPC Endpoints (No NAT Gateway) | $7-$20                    |
| Traffic    | CloudFront distribution              | $10-$30                   |
| Monitoring | CloudWatch metrics, logs & alarms    | $5-$10                    |

Total Primary Region:
ğŸ‘‰ $187â€“$305 per month

DR Region (Warm Standby) â€” Estimated Monthly Cost
| Component       | Service                              | Cost Behavior           | Approx Monthly Cost       |
| --------------- |--------------------------------------|-------------------------|---------------------------|
| Compute         | ECS Fargate tasks                    | Scaled to 0 (normal)    | $0                        |
| Database        | RDS cross-region read replica        | reqiured                | $120â€“$150                 |  
| Storage         | S3 replication target                | Minimal                 | $3-$10                    |
| Networking      | ALB + VPC Endpoints (No NAT Gateway) | Low                     | $18-$30                   |
| Traffic         | CloudFront distribution              | Shared                  | $0                        |
| Logs/Monitoring | CloudWatch                           | Small volume            | $3-$6                     |
Note:
ECS tasks in the DR region are scaled to zero during normal operation.
They scale up to two tasks only during a failover event, so DR compute cost is effectively zero until activation.

Total DR Region:
ğŸ‘‰ $145â€“$196 per month

Total Multi-Region Cost
Primary ($187â€“$305) + DR ($145â€“$196)
ğŸ‘‰ Estimated Total: $332â€“$501 per month

---

# âš ï¸ **Known Limitations and Trade-offs**

This project implements a realistic multi-region DR (Disaster Recovery) architecture using a Warm Standby strategy. While effective and cost-efficient, it includes several intentional trade-offs and limitations that are important to understand.

### 1. Manual RDS Failover (Replica Promotion)

* The cross-region RDS replica does not automatically become primary.
* In a region-wide failure, an operator must manually promote the read replica in the DR region.
* This introduces a small delay (RTO) until the database becomes writable again.

  Trade-off:
  Automatic failover reduces downtime but increases complexity and cost. Manual promotion is simpler and appropriate for warm-standby DR.

### 2. RDS Replication Lag (RPO > 0)

* Cross-region MySQL replication introduces replication lag of seconds to minutes depending on load.
* During failover, the DR region may lose very recent writes.

  Trade-off:
  Achieving zero data loss requires synchronous replication or multi-master setups, which are significantly more expensive.

### 3. ECS DR Cluster is Warm, Not Active

* ECS tasks in the DR region are scaled down to 0 (or minimal) until failover.
* A failover event requires scaling ECS services up, which adds delay before full recovery.

  Trade-off:
  Warm standby reduces cost by 50â€“70% compared to active-active multi-region setups.

### 4. Health checks do not validate the database layer

* CloudFront + ALB health checks validate HTTP availability, not database availability.
* If ALB/ECS is healthy but RDS is down, users may still see WordPress errors.

  Trade-off: End-to-end DB health checks require custom application endpoints or deeper monitoring.
  For simplicity, this solution checks only at the HTTP level.

### 5. Lambda Automation Runs Once (Bootstrap Only)

* The DB setup Lambda runs only during initial deployment.
* Schema migrations or future DB updates must be handled manually or using a CI/CD process.

  Trade-off:
  Automating migrations adds complexity (Liquibase, Flyway, custom pipelines), so bootstrap-only logic keeps the project simple.

### 6. S3 Replication Provides Eventual Consistency

* S3 Cross-Region Replication (CRR) is asynchronous.
* Media files may take seconds or minutes to appear in the DR region after upload.

  Trade-off:
  Fully synchronous replication for media storage is expensive and not supported natively by S3.

### 7. Failover Decision Is Operator-Driven

* This DR design intentionally avoids auto-failover to prevent false positives.
* Failover requires human confirmation.

  Trade-off:
  Auto-failover is fast but risks flipping regions due to transient issues. Warm-standby DR normally uses controlled manual failover.

### 8. No Fully Automated DR Drills

* Disaster Recovery drills (regional failure simulations) must be executed manually.
* Automated DR testing pipelines are not included.

  Trade-off:
  Automation adds complexity; manual testing remains common in warm standby setups.

### 9. DR ALB Always Running

* The DR region's ALB is always provisioned to allow immediate failover.
* This introduces a small fixed cost even when the region is idle.

  Trade-off:
  Keeping the ALB "cold" would save money but increases RTO (longer DR recovery time).

### 10. IAM & RDS Secrets Manager â€“ Region-Specific Consistency Trade-off
This limitation documents a region-specific consistency behavior observed when using
Amazon RDSâ€“managed Secrets Manager credentials together with tag-based IAM conditions.

While tag-based access control worked reliably in `us-east-1`, it caused non-deterministic
failures in other regions (e.g. `us-east-2`) due to eventual consistency in AWS-managed
secret creation and tag propagation.

As a result, this project intentionally prioritizes deterministic, region-safe IAM policies
over tag-based conditions for RDS-managed secrets.

#### Background

This project uses Amazon RDS with `manage_master_user_password = true`, which instructs AWS
to automatically generate and manage the database master credentials in AWS Secrets Manager.

The secret is created and owned by Amazon RDS, not by Terraform or the application.

---

Initial Security Approach (Tag-Based IAM Conditions)

To follow least-privilege IAM principles, the Lambda function responsible for initializing the WordPress database was originally restricted using resource tags on the RDS-generated secret.

Example IAM policy (initial attempt)

```bash 
# RDS auth secret (master password)
{
  Effect = "Allow"
  Action = [
    "secretsmanager:GetSecretValue",
    "secretsmanager:DescribeSecret"
  ]
  Resource = ["arn:aws:secretsmanager:${var.primary_region}:${data.aws_caller_identity.current.account_id}:secret:*"],
  Condition = {
    StringEquals = {
      "aws:ResourceTag/Project" = "wordpress",
      "aws:ResourceTag/Component" = "rds-auth"
    }
  }
}
```


Terraform attempted to tag the RDS-managed secret immediately after the database was created:

```bash
resource "null_resource" "tag_rds_master_secret" {
  triggers = {
    secret = aws_db_instance.rds.master_user_secret[0].secret_arn
  }

  provisioner "local-exec" {
    command = <<EOT
aws secretsmanager tag-resource \
  --secret-id ${aws_db_instance.rds.master_user_secret[0].secret_arn} \
  --tags Key=Project,Value=wordpress Key=Component,Value=rds-auth
EOT
  }
}
```
--- 

Observed Behavior

âœ… In us-east-1: 

  * This approach worked reliably across multiple deployments.

âŒ In us-east-2 (and other regions): 

  * Terraform failed with the following error:

```bash
An error occurred (ResourceNotFoundException) when calling the TagResource operation:
Secrets Manager can't find the specified secret.
```

This occurred even though:

  * The secret did exist

  * The secret was visible in the AWS Console

  * The ARN was correct

  * The RDS instance had completed creation

---

Root Cause (AWS Service Behavior)

This is a region-dependent AWS behavior, not a Terraform bug.

Key facts:

  * RDS-managed secrets are created asynchronously

  * Tag propagation and tag visibility are eventually consistent

  * AWS does not guarantee immediate tag availability for service-managed secrets

  * IAM tag-based conditions may be evaluated before tags are visible

  * Behavior is not consistent across regions

As a result:

  * Lambda was denied access because the tag condition could not be evaluated

  * Even though the secret existed, AWS treated it as not matching the IAM policy

  * This caused non-deterministic failures outside us-east-1

---

Final Design Decision (Production-Safe)

To ensure deterministic and region-safe deployments, the project intentionally does not rely on tag-based IAM conditions for RDS-managed secrets.

Instead, access is restricted by:

  * Account

  * Region

  * Secret ARN pattern

Final IAM policy (used in this project)
```bash
  # RDS auth secret (master password)
  {
    Effect = "Allow"
    Action = [
      "secretsmanager:GetSecretValue",
      "secretsmanager:DescribeSecret"
    ]
    Resource = ["arn:aws:secretsmanager:${var.primary_region}:${data.aws_caller_identity.current.account_id}:secret:*"]
  }
```
Why This Is the Correct Approach

âœ” Deterministic across all AWS regions

âœ” No race conditions during provisioning

âœ” Works reliably in CI/CD pipelines

âœ” Still enforces strong isolation (account + region scoped)

âœ” Aligns with AWS ownership model for service-managed resources

AWS Well-Architected guidance favors reliability and predictability over brittle, timing-sensitive IAM conditions â€” especially for resources created and controlled by AWS services.

---

Summary

These trade-offs make the solution:
Reliable
Realistic
Cost-effective

Aligned with AWS Well-Architected DR patterns (Warm Standby)

But they also mean the system is not active-active and sacrifices some speed and automation in exchange for simplicity and affordability.

--- 

# ğŸ“„ **License**

This project is open for personal and educational use.

---



